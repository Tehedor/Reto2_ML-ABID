{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d999ed5b",
   "metadata": {},
   "source": [
    "<header style=\"width:100%;position:relative\">\n",
    "  <div style=\"width:80%;float:right;\">\n",
    "    <h1>False Political Claim Detection</h1>\n",
    "    <h3>Carga y limpieza de los datos - LLM</h3>\n",
    "    <h5>Grupo 2</h5>\n",
    "  </div>\n",
    "        <img style=\"width:15%;\" src=\"./images/logo.jpg\" alt=\"UPM\" />\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34585ec",
   "metadata": {},
   "source": [
    "# Índice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee7cb30",
   "metadata": {},
   "source": [
    "1. [Importar librerias](#1.-Importar-librerias)  \n",
    "2. [Funciones auxiliares](#2.-Funciones-auxiliares)  \n",
    "3. [Configuracion de los diccionarios](#3.-configuracion-de-los-diccionarios)  \n",
    "4. [Referencias](#4-referencias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69127d82",
   "metadata": {},
   "source": [
    "# 1. Importar Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95c8663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "from multiprocessing import process\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# pip install gensim\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# pip install pyLDAvis==3.4.1\n",
    "# import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8874711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collection(df, label, column):\n",
    "    collection = []\n",
    "    \n",
    "    for x in df[df['label'] == label][column].str.split():\n",
    "        for i in x:\n",
    "            collection.append(i)\n",
    "    return collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b7f402ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json(outputfile, dict):\n",
    "    with open(outputfile, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dict, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80e65b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables generales\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "en_stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2d083",
   "metadata": {},
   "source": [
    "# 2. Carga del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3ab186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "url = \"formated/train_exportado.csv\" \n",
    "df_train_exportado = pd.read_csv(url)\n",
    "\n",
    "\n",
    "# Test\n",
    "url2 = \"formated/test_exportado.csv\"\n",
    "df_test_exportado = pd.read_csv(url2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b52f0",
   "metadata": {},
   "source": [
    "# 3. Configuracion de los diccionarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97798c",
   "metadata": {},
   "source": [
    "## 3.1 Quitando los comunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6d21e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "statement_train = df_train_exportado['statement-lemmatize']\n",
    "statement_test = df_test_exportado['statement-lemmatize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c90af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de valores mínimos de frecuencia. Una palabra debe aparecer al menos este número de veces en los documentos para ser incluida en el diccionario.\n",
    "min_freq_values = [5, 10, 20, 50]\n",
    "\n",
    "#  Lista de proporciones máximas. Una palabra no debe aparecer en más del porcentaje especificado de documentos para ser incluida en el diccionario.\n",
    "max_prop_values = [0.5, 0.65, 0.8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c65c44b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Diccionarios guardados para min_freq=5, max_prop=0.5\n",
      "✔ Diccionarios guardados para min_freq=5, max_prop=0.65\n",
      "✔ Diccionarios guardados para min_freq=5, max_prop=0.8\n",
      "✔ Diccionarios guardados para min_freq=10, max_prop=0.5\n",
      "✔ Diccionarios guardados para min_freq=10, max_prop=0.65\n",
      "✔ Diccionarios guardados para min_freq=10, max_prop=0.8\n",
      "✔ Diccionarios guardados para min_freq=20, max_prop=0.5\n",
      "✔ Diccionarios guardados para min_freq=20, max_prop=0.65\n",
      "✔ Diccionarios guardados para min_freq=20, max_prop=0.8\n",
      "✔ Diccionarios guardados para min_freq=50, max_prop=0.5\n",
      "✔ Diccionarios guardados para min_freq=50, max_prop=0.65\n",
      "✔ Diccionarios guardados para min_freq=50, max_prop=0.8\n"
     ]
    }
   ],
   "source": [
    "# Tokeniza\n",
    "tokenized_train = [str(text).split() for text in statement_train]\n",
    "tokenized_test = [str(text).split() for text in statement_test]\n",
    "\n",
    "# Cuenta palabras\n",
    "counter_train = Counter(word for doc in tokenized_train for word in doc)\n",
    "counter_test = Counter(word for doc in tokenized_test for word in doc)\n",
    "\n",
    "n_docs_train = len(tokenized_train)\n",
    "n_docs_test = len(tokenized_test)\n",
    "\n",
    "# Cuenta en cuántos documentos aparece cada palabra (frecuencia de documento, no total)\n",
    "def document_frequency(tokenized_docs):\n",
    "    df_counter = Counter()\n",
    "    for doc in tokenized_docs:\n",
    "        unique_words = set(doc)\n",
    "        df_counter.update(unique_words)\n",
    "    return df_counter\n",
    "\n",
    "doc_freq_train = document_frequency(tokenized_train)\n",
    "doc_freq_test = document_frequency(tokenized_test)\n",
    "\n",
    "# Función para filtrar un diccionario de palabras\n",
    "def filtra(dic_frecuencia, doc_frecuencia, total_docs, min_freq, max_prop):\n",
    "    return {\n",
    "        word: dic_frecuencia[word]\n",
    "        for word in dic_frecuencia\n",
    "        if doc_frecuencia[word] >= min_freq and (doc_frecuencia[word] / total_docs) <= max_prop\n",
    "    }\n",
    "\n",
    "# Procesa para cada combinación\n",
    "for min_freq in min_freq_values:\n",
    "    for max_prop in max_prop_values:\n",
    "        # Filtrado\n",
    "        filtered_train = filtra(counter_train, doc_freq_train, n_docs_train, min_freq, max_prop)\n",
    "        filtered_test = filtra(counter_test, doc_freq_test, n_docs_test, min_freq, max_prop)\n",
    "\n",
    "        # Palabras en común\n",
    "        common_words = set(filtered_train.keys()) & set(filtered_test.keys())\n",
    "\n",
    "        # Exclusivos\n",
    "        exclusive_train = {word: freq for word, freq in filtered_train.items() if word not in common_words}\n",
    "        exclusive_test = {word: freq for word, freq in filtered_test.items() if word not in common_words}\n",
    "\n",
    "        # Guarda todo como JSON\n",
    "        base = f\"nb{min_freq}_na{int(max_prop * 100)}\"\n",
    "\n",
    "        with open(f\"dictionaries/train_{base}.json\", \"w\") as f:\n",
    "            json.dump(filtered_train, f, indent=2)\n",
    "        with open(f\"dictionaries/test_{base}.json\", \"w\") as f:\n",
    "            json.dump(filtered_test, f, indent=2)\n",
    "        with open(f\"dictionaries/train_exclusive_{base}.json\", \"w\") as f:\n",
    "            json.dump(exclusive_train, f, indent=2)\n",
    "        with open(f\"dictionaries/test_exclusive_{base}.json\", \"w\") as f:\n",
    "            json.dump(exclusive_test, f, indent=2)\n",
    "\n",
    "        print(f\"✔ Diccionarios guardados para min_freq={min_freq}, max_prop={max_prop}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bd022d",
   "metadata": {},
   "source": [
    "## 3.2 Otra manera"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3312055d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "df_train_exportado.columns"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fe2aff1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "statement_train = df_train_exportado['statement-lemmatize']\n",
    "statement_test = df_test_exportado['statement-lemmatize']\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37c61712",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# # Drop rows with missing values in the 'processed_review' column\n",
    "\n",
    "\n",
    "# input_phrases = [\n",
    "#     \"This is a test phrase\",\n",
    "#     \"Another test test example of a political statement\",\n",
    "#     \"Machine learning is fascinating\",\n",
    "#     \"Natural language processing is a subfield of AI\",\n",
    "#     \"I love programming in Python\",\n",
    "# ]\n",
    "\n",
    "\n",
    "# # Tokenize the processed reviews\n",
    "# tokenized_reviews = [str(review).split() for review in input_phrases]\n",
    "# # tokenized_reviews = [str(review).split() for review in statement_train]\n",
    "\n",
    "# # Create dictionary of tokens\n",
    "# D = gensim.corpora.Dictionary(tokenized_reviews)\n",
    "# n_tokens = len(D)\n",
    "\n",
    "# print('The dictionary contains', n_tokens, 'terms')\n",
    "# print('First terms in the dictionary:')\n",
    "# for n in range(20):\n",
    "#     print(str(n), ':', D[n])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "50dcc192",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# # Define un umbral mínimo: una palabra debe aparecer en al menos 100 documentos para ser incluida en el diccionario.\n",
    "# # Esto elimina palabras raras o poco frecuentes que no aportan mucho valor al análisis.\n",
    "# no_below = 20 # Minimum number of documents to keep a term in the dictionary\n",
    "\n",
    "# # Define un umbral máximo: una palabra no debe aparecer en más del 65% de los documentos para ser incluida en el diccionario.\n",
    "# # Esto elimina palabras demasiado comunes (como \"the\", \"and\", etc.) que no son útiles para diferenciar documentos.\n",
    "# no_above = .65 # Maximum proportion of documents in which a term can appear to be kept in the dictionary\n",
    "\n",
    "# D.filter_extremes(no_below=no_below,no_above=no_above)\n",
    "\n",
    "\n",
    "# n_tokens = len(D)\n",
    "\n",
    "# print('The dictionary contains', n_tokens, 'terms')\n",
    "\n",
    "# print('First terms in the dictionary:')\n",
    "# for n in range(10):\n",
    "#     print(str(n), ':', D[n])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64e93e94",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Tokeniza las frases ya limpiadas y lematizadas\n",
    "tokenized_reviews = [str(review).split() for review in statement_train]\n",
    "\n",
    "# Parámetros a probar\n",
    "no_below_values = [5, 10, 20, 50]\n",
    "no_above_values = [0.5, 0.65, 0.8]\n",
    "\n",
    "# Crear múltiples diccionarios con diferentes combinaciones\n",
    "for no_below in no_below_values:\n",
    "    for no_above in no_above_values:\n",
    "        D = Dictionary(tokenized_reviews)\n",
    "        D.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "        n_tokens = len(D)\n",
    "        print(f\"Diccionario (min={no_below}, max={no_above}): {n_tokens} términos\")\n",
    "\n",
    "        # Guarda el diccionario\n",
    "        fname = f\"dictionaries/dict_nb{no_below}_na{int(no_above*100)}.dict\"\n",
    "        D.save(fname)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b723c7c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Carga el diccionario elegido\n",
    "D = Dictionary.load(\"dictionaries/dict_nb10_na65.dict\")\n",
    "\n",
    "# Tokeniza test\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "tokenized_test = [str(review).split() for review in statement_test]\n",
    "\n",
    "# Convierte a bolsa de palabras (solo se usarán palabras conocidas)\n",
    "corpus_test = [D.doc2bow(tokens) for tokens in tokenized_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13caf815",
   "metadata": {},
   "source": [
    "# 4. Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77052133",
   "metadata": {},
   "source": [
    "* [pandas documentation — pandas 2.2.3 documentation. (s. f.).](https://pandas.pydata.org/docs/)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda3_r2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
