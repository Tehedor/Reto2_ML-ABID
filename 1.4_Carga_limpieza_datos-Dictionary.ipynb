{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d999ed5b",
   "metadata": {},
   "source": [
    "<header style=\"width:100%;position:relative\">\n",
    "  <div style=\"width:80%;float:right;\">\n",
    "    <h1>False Political Claim Detection</h1>\n",
    "    <h3>Carga y limpieza de los datos - LLM</h3>\n",
    "    <h5>Grupo 2</h5>\n",
    "  </div>\n",
    "        <img style=\"width:15%;\" src=\"./images/logo.jpg\" alt=\"UPM\" />\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34585ec",
   "metadata": {},
   "source": [
    "# Índice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee7cb30",
   "metadata": {},
   "source": [
    "1. [Importar librerias](#1.-Importar-librerias)  \n",
    "2. [Funciones auxiliares](#2.-Funciones-auxiliares)  \n",
    "3. [Configuracion de los diccionarios](#3.-configuracion-de-los-diccionarios)  \n",
    "4. [Creacion de One-Hot](#4.-Creacion-de-One-Hot)  \n",
    "5. [Exportar CSV](#5.-Exportar-CSV)  \n",
    "6. [Referencias](#6-referencias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69127d82",
   "metadata": {},
   "source": [
    "# 1. Importar Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "95c8663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "from multiprocessing import process\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# pip install gensim\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# pip install pyLDAvis==3.4.1\n",
    "# import pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2d083",
   "metadata": {},
   "source": [
    "# 2. Carga del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a3ab186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "url = \"formated/train_exportado.csv\" \n",
    "df_train_exportado = pd.read_csv(url)\n",
    "\n",
    "\n",
    "# Test\n",
    "url2 = \"formated/test_exportado.csv\"\n",
    "df_test_exportado = pd.read_csv(url2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b52f0",
   "metadata": {},
   "source": [
    "# 3. Configuracion de los diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d21e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = 'statement-lemmatize'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4c90af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de valores mínimos de frecuencia. Una palabra debe aparecer al menos este número de veces en los documentos para ser incluida en el diccionario.\n",
    "# min_freq_values = [5, 10, 20, 50]\n",
    "min_freq_values = [10, 20]\n",
    "\n",
    "# Lista de proporciones máximas. Una palabra no debe aparecer en más del porcentaje especificado de documentos para ser incluida en el diccionario.\n",
    "# max_prop_values = [0.5, 0.65, 0.8]\n",
    "max_prop_values = [0.8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a945459a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5795\n",
      "3155\n"
     ]
    }
   ],
   "source": [
    "# Separar en dos listas según la etiqueta\n",
    "statements_true = [str(text).split() for text, label in zip(df_train_exportado[statement], df_train_exportado['label']) if label == 1]\n",
    "statements_false =[str(text).split() for text, label in zip(df_train_exportado[statement], df_train_exportado['label']) if label == 0]\n",
    "\n",
    "print(len(statements_true))\n",
    "print(len(statements_false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c65c44b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Guardado con min_freq_true=10, min_freq_false=5, max_prop=0.8\n",
      "✔ Guardado con min_freq_true=20, min_freq_false=11, max_prop=0.8\n"
     ]
    }
   ],
   "source": [
    "# Cuenta en cuántos documentos aparece cada palabra\n",
    "def document_frequency(tokenized_docs):\n",
    "    df_counter = Counter()\n",
    "    for doc in tokenized_docs:\n",
    "        unique_words = set(doc)\n",
    "        df_counter.update(unique_words)\n",
    "    return df_counter\n",
    "\n",
    "# Función para filtrar palabras por frecuencia mínima y máxima proporción\n",
    "def filtra(dic_frecuencia, doc_frecuencia, total_docs, min_freq, max_prop):\n",
    "    return {\n",
    "        word: dic_frecuencia[word]  # frecuencia total (recuento)\n",
    "        for word in dic_frecuencia\n",
    "        if doc_frecuencia[word] >= min_freq and (doc_frecuencia[word] / total_docs) <= max_prop\n",
    "    }\n",
    "\n",
    "# Tokenización según clase\n",
    "statements_true = [str(text).split() for text, label in zip(df_train_exportado[statement], df_train_exportado['label']) if label == 1]\n",
    "statements_false = [str(text).split() for text, label in zip(df_train_exportado[statement], df_train_exportado['label']) if label == 0]\n",
    "\n",
    "# Estadísticas base\n",
    "n_docs_true = len(statements_true)\n",
    "n_docs_false = len(statements_false)\n",
    "adjust_ratio = n_docs_true / n_docs_false\n",
    "\n",
    "# Contadores\n",
    "counter_true = Counter(word for doc in statements_true for word in doc)\n",
    "counter_false = Counter(word for doc in statements_false for word in doc)\n",
    "df_true = document_frequency(statements_true)\n",
    "df_false = document_frequency(statements_false)\n",
    "\n",
    "# Procesa combinaciones\n",
    "for min_freq_true in min_freq_values:\n",
    "    min_freq_false = max(1, round(min_freq_true / adjust_ratio))  # Ajuste proporcional\n",
    "\n",
    "    for max_prop in max_prop_values:\n",
    "        # Filtrado\n",
    "        filtered_true = filtra(counter_true, df_true, n_docs_true, min_freq_true, max_prop)\n",
    "        filtered_false = filtra(counter_false, df_false, n_docs_false, min_freq_false, max_prop)\n",
    "\n",
    "        # Palabras en común\n",
    "        common_words = set(filtered_true.keys()) & set(filtered_false.keys())\n",
    "\n",
    "        # Exclusivos (frecuencia absoluta)\n",
    "        exclusive_true = {\n",
    "            word: freq for word, freq in filtered_true.items() if word not in common_words\n",
    "        }\n",
    "        exclusive_false = {\n",
    "            word: freq for word, freq in filtered_false.items() if word not in common_words\n",
    "        }\n",
    "\n",
    "        # Opcional: ordenar por frecuencia descendente\n",
    "        filtered_true = dict(sorted(filtered_true.items(), key=lambda x: x[1], reverse=True))\n",
    "        filtered_false = dict(sorted(filtered_false.items(), key=lambda x: x[1], reverse=True))\n",
    "        exclusive_true = dict(sorted(exclusive_true.items(), key=lambda x: x[1], reverse=True))\n",
    "        exclusive_false = dict(sorted(exclusive_false.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        # Guarda resultados\n",
    "        base = f\"nb{min_freq_true}_na{int(max_prop * 100)}\"\n",
    "        with open(f\"json/dictionaries/true_{base}.json\", \"w\") as f:\n",
    "            json.dump(filtered_true, f, indent=2)\n",
    "        with open(f\"json/dictionaries/false_{base}.json\", \"w\") as f:\n",
    "            json.dump(filtered_false, f, indent=2)\n",
    "        with open(f\"json/dictionaries/true_exclusive_{base}.json\", \"w\") as f:\n",
    "            json.dump(exclusive_true, f, indent=2)\n",
    "        with open(f\"json/dictionaries/false_exclusive_{base}.json\", \"w\") as f:\n",
    "            json.dump(exclusive_false, f, indent=2)\n",
    "\n",
    "        print(f\"✔ Guardado con min_freq_true={min_freq_true}, min_freq_false={min_freq_false}, max_prop={max_prop}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a230f267",
   "metadata": {},
   "source": [
    "# 4. Creacion de One-Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "31355414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "\n",
    "def one_hot_encode_statements(statements_raw, selected_words):\n",
    "    \"\"\"\n",
    "    Genera una codificación One-Hot para los statements, basada en un conjunto de palabras seleccionadas.\n",
    "\n",
    "    Args:\n",
    "        statements_raw (list of str): Lista de frases originales.\n",
    "        selected_words (set): Conjunto de palabras seleccionadas.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame One-Hot codificado.\n",
    "    \"\"\"\n",
    "    # Agregar prefijo 'word_' a cada palabra seleccionada\n",
    "    prefixed_words = {f\"word_{word}\" for word in selected_words}\n",
    "\n",
    "    # Inicializa lista para cada fila codificada\n",
    "    encoded_rows = []\n",
    "\n",
    "    for statement in statements_raw:\n",
    "        tokens = set(str(statement).split())  # Convierte en set para rapidez\n",
    "        encoded_row = {f\"word_{word}\": int(word in tokens) for word in selected_words}\n",
    "        encoded_rows.append(encoded_row)\n",
    "\n",
    "    # Crea DataFrame\n",
    "    df_one_hot = pd.DataFrame(encoded_rows)\n",
    "    df_one_hot.fillna(0, inplace=True)  # Por si alguna palabra no se encuentra\n",
    "\n",
    "    return df_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a3b4cc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_true = 'true_exclusive_nb20_na80'\n",
    "dictionary_false = 'false_exclusive_nb20_na80'\n",
    "\n",
    "# Carga los diccionarios true y false\n",
    "with open(f\"json/dictionaries/{dictionary_true}.json\", 'r') as f:\n",
    "    dict_true = json.load(f)\n",
    "\n",
    "with open(f\"json/dictionaries/{dictionary_false}.json\", 'r') as f:\n",
    "    dict_false = json.load(f)\n",
    "\n",
    "# Combina ambos diccionarios\n",
    "complete_dictionary = set({**dict_true, **dict_false}.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d8b3edb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_one_hot = one_hot_encode_statements(df_train_exportado[statement], complete_dictionary)\n",
    "df_test_one_hot = one_hot_encode_statements(df_test_exportado[statement], complete_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "70004918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['word_previous', 'word_commit', 'word_worth', 'word_face',\n",
      "       'word_things', 'word_johnson', 'word_statewide', 'word_policies',\n",
      "       'word_presidents', 'word_amount',\n",
      "       ...\n",
      "       'word_rise', 'word_some', 'word_criminal', 'word_officers', 'word_list',\n",
      "       'word_union', 'word_cities', 'word_improve', 'word_fewer',\n",
      "       'word_providence'],\n",
      "      dtype='object', length=180)\n"
     ]
    }
   ],
   "source": [
    "print(df_train_one_hot.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cca170",
   "metadata": {},
   "source": [
    "# 5. Exportar CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4f443e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "url = \"formated/train_exportado.csv\" \n",
    "df_train = pd.read_csv(url)\n",
    "\n",
    "\n",
    "# Test\n",
    "url2 = \"formated/test_exportado.csv\"\n",
    "df_test = pd.read_csv(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "757f0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar los datasets originales con sus correspondientes codificaciones one-hot\n",
    "df_train_combined = pd.concat([df_train, df_train_one_hot], axis=1)\n",
    "df_test_combined = pd.concat([df_test, df_test_one_hot], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "322e44f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existen otras diferencias en las columnas: {'label'}\n",
      "Columnas en train pero no en test: {'label'}\n",
      "Columnas en test pero no en train: set()\n"
     ]
    }
   ],
   "source": [
    "# Comparar las columnas de dfc y dfa\n",
    "columns_train = set(df_train_combined.columns)\n",
    "columns_test = set(df_test_combined.columns)\n",
    "\n",
    "# Encontrar las diferencias\n",
    "differences = columns_train.symmetric_difference(columns_test)\n",
    "\n",
    "# Verificar si la única diferencia es 'id' y 'label'\n",
    "if differences == {'id', 'label'}:\n",
    "    print(\"La única diferencia entre las columnas es 'id' y 'label'.\")\n",
    "else:\n",
    "    print(\"Existen otras diferencias en las columnas:\", differences)\n",
    "    print(\"Columnas en train pero no en test:\", columns_train - columns_test)\n",
    "    print(\"Columnas en test pero no en train:\", columns_test - columns_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dce0b33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets combinados y exportados correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Exportar los datasets combinados a archivos CSV\n",
    "df_train_combined.to_csv('./formated/train_exportado_words.csv', index=False)\n",
    "df_test_combined.to_csv('./formated/test_exportado_words.csv', index=False)\n",
    "\n",
    "print(\"Datasets combinados y exportados correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13caf815",
   "metadata": {},
   "source": [
    "# 5. Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77052133",
   "metadata": {},
   "source": [
    "* [pandas documentation — pandas 2.2.3 documentation. (s. f.).](https://pandas.pydata.org/docs/)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda3_r2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
