{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d999ed5b",
   "metadata": {},
   "source": [
    "<header style=\"width:100%;position:relative\">\n",
    "  <div style=\"width:80%;float:right;\">\n",
    "    <h1>False Political Claim Detection</h1>\n",
    "    <h3>Carga y limpieza de los datos - LLM</h3>\n",
    "    <h5>Grupo 2</h5>\n",
    "  </div>\n",
    "        <img style=\"width:15%;\" src=\"./images/logo.jpg\" alt=\"UPM\" />\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34585ec",
   "metadata": {},
   "source": [
    "# Índice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee7cb30",
   "metadata": {},
   "source": [
    "1. [Importar librerias](#1.-Importar-librerias)  \n",
    "2. [Funciones auxiliares](#2.-Funciones-auxiliares)  \n",
    "3. [Configuracion de los diccionarios](#3.-configuracion-de-los-diccionarios)  \n",
    "4. [Creacion de One-Hot](#4.-Creacion-de-One-Hot)  \n",
    "5. [Exportar CSV](#5.-Exportar-CSV)  \n",
    "6. [Referencias](#6-referencias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69127d82",
   "metadata": {},
   "source": [
    "# 1. Importar Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c8663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "from multiprocessing import process\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# pip install gensim\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# pip install pyLDAvis==3.4.1\n",
    "# import pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2d083",
   "metadata": {},
   "source": [
    "# 2. Carga del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "url = \"formated/train_exportado.csv\" \n",
    "df_train_exportado = pd.read_csv(url)\n",
    "\n",
    "\n",
    "# Test\n",
    "url2 = \"formated/test_exportado.csv\"\n",
    "df_test_exportado = pd.read_csv(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_exportado_words_onehot = df_train_exportado[['id', 'label']]\n",
    "df_test_exportado_words_onehot = df_test_exportado[['id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b52f0",
   "metadata": {},
   "source": [
    "# 3. Configuracion de los diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d21e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = 'statement-lemmatize'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a0d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")  # Carga el modelo de spaCy\n",
    "\n",
    "def clean_statements(statements):\n",
    "    global nlp\n",
    "    \"\"\"\n",
    "    Elimina pronombres, palabras vacías, palabras irrelevantes y tokens de longitud 1 de las declaraciones.\n",
    "\n",
    "    Args:\n",
    "        statements (list of str): Lista de declaraciones a limpiar.\n",
    "        nlp (spacy.Language): Modelo de spaCy cargado.\n",
    "\n",
    "    Returns:\n",
    "        list of str: Lista de declaraciones limpias.\n",
    "    \"\"\"\n",
    "    # Combina las stop words de spaCy con las personalizadas\n",
    "    custom_stopwords = {\"i\", \"we\", \"it\", \"he\", \"she\", \"they\", \"our\", \"that\", \"have\", \"be\", \"say\"}\n",
    "    all_stopwords = nlp.Defaults.stop_words.union(custom_stopwords)\n",
    "\n",
    "    cleaned_statements = []\n",
    "    for statement in statements:\n",
    "        doc = nlp(statement)\n",
    "        cleaned_tokens = []\n",
    "        for token in doc:\n",
    "            # Excluir palabras que son stop words, están en la lista personalizada o tienen longitud 1\n",
    "            if token.text.lower() in all_stopwords or token.is_stop or len(token.text) == 1 or len(token.text) == 2:\n",
    "                continue\n",
    "            # Incluir solo palabras relevantes\n",
    "            if token.pos_ in {\"VERB\", \"NOUN\", \"PROPN\", \"ADJ\"}:\n",
    "                cleaned_tokens.append(token.text.lower())\n",
    "        cleaned_statements.append(\" \".join(cleaned_tokens))\n",
    "    return cleaned_statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acfeb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d9794",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_exportado[statement] = clean_statements(df_train_exportado[statement]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_exportado[statement] = clean_statements(df_test_exportado[statement]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b89089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train_exportado[statement].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aec819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_array_serie(inicio, fin):\n",
    "  \"\"\"\n",
    "  Genera un array que sigue la serie 4, 7, 11, 16, 22... hasta alcanzar o superar el valor final.\n",
    "\n",
    "  Args:\n",
    "    inicio: El primer número de la serie.\n",
    "    fin: El valor máximo que el último elemento del array no debe superar.\n",
    "\n",
    "  Returns:\n",
    "    Una lista (array) con los números de la serie.\n",
    "  \"\"\"\n",
    "  array = [inicio]\n",
    "  ultimo_elemento = inicio\n",
    "#   incremento = 3\n",
    "  incremento = 1\n",
    "  while ultimo_elemento < fin:\n",
    "    ultimo_elemento += incremento\n",
    "    array.append(ultimo_elemento)\n",
    "    incremento += 1\n",
    "  return array\n",
    "\n",
    "mi_array = generar_array_serie(4, 270)\n",
    "# mi_array = generar_array_serie(2, 270)\n",
    "print(mi_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c90af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de valores mínimos de frecuencia. Una palabra debe aparecer al menos este número de veces en los documentos para ser incluida en el diccionario.\n",
    "# min_freq_values = [5, 10, 20, 50]\n",
    "# min_freq_values = [10, 20, 30, 40 ,50]\n",
    "# min_freq_values = [5,10 ,20, 40, 60 ,80,100]\n",
    "# min_freq_values = [4,7,10 ,20, 40, 60 ,80,100]\n",
    "# min_freq_values = [4,7,10 ,20, 35, 55 ,80,105, 135]\n",
    "min_freq_values =   mi_array   \n",
    "\n",
    "\n",
    "# Lista de proporciones máximas. Una palabra no debe aparecer en más del porcentaje especificado de documentos para ser incluida en el diccionario.\n",
    "# max_prop_values = [0.5, 0.65, 0.8]\n",
    "max_prop_values = [0.4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a945459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar en dos listas según la etiqueta\n",
    "statements_true = [str(text).split() for text, label in zip(df_train_exportado[statement], df_train_exportado['label']) if label == 1]\n",
    "statements_false =[str(text).split() for text, label in zip(df_train_exportado[statement], df_train_exportado['label']) if label == 0]\n",
    "\n",
    "print(len(statements_true))\n",
    "print(len(statements_false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuenta en cuántos documentos aparece cada palabra\n",
    "def document_frequency(tokenized_docs):\n",
    "    df_counter = Counter()\n",
    "    for doc in tokenized_docs:\n",
    "        unique_words = set(doc)\n",
    "        df_counter.update(unique_words)\n",
    "    return df_counter\n",
    "\n",
    "# Función para filtrar palabras por frecuencia mínima y máxima proporción\n",
    "def filtra(dic_frecuencia, doc_frecuencia, total_docs, min_freq, max_prop):\n",
    "    return {\n",
    "        word: dic_frecuencia[word]  # frecuencia total (recuento)\n",
    "        for word in dic_frecuencia\n",
    "        if doc_frecuencia[word] >= min_freq and (doc_frecuencia[word] / total_docs) <= max_prop\n",
    "    }\n",
    "\n",
    "# Tokenización según clase\n",
    "statements_true = [str(text).split() for text, label in zip(df_train_exportado[statement], df_train_exportado['label']) if label == 1]\n",
    "statements_false = [str(text).split() for text, label in zip(df_train_exportado[statement], df_train_exportado['label']) if label == 0]\n",
    "\n",
    "# Estadísticas base\n",
    "n_docs_true = len(statements_true)\n",
    "n_docs_false = len(statements_false)\n",
    "adjust_ratio = n_docs_true / n_docs_false\n",
    "\n",
    "# Contadores\n",
    "counter_true = Counter(word for doc in statements_true for word in doc)\n",
    "counter_false = Counter(word for doc in statements_false for word in doc)\n",
    "df_true = document_frequency(statements_true)\n",
    "df_false = document_frequency(statements_false)\n",
    "\n",
    "# Procesa combinaciones\n",
    "for min_freq_true in min_freq_values:\n",
    "    min_freq_false = max(1, round(min_freq_true / adjust_ratio))  # Ajuste proporcional\n",
    "\n",
    "    for max_prop in max_prop_values:\n",
    "        # Filtrado\n",
    "        filtered_true = filtra(counter_true, df_true, n_docs_true, min_freq_true, max_prop)\n",
    "        filtered_false = filtra(counter_false, df_false, n_docs_false, min_freq_false, max_prop)\n",
    "\n",
    "        # Palabras en común\n",
    "        common_words = set(filtered_true.keys()) & set(filtered_false.keys())\n",
    "\n",
    "        # Exclusivos (frecuencia absoluta)\n",
    "        exclusive_true = {\n",
    "            word: freq for word, freq in filtered_true.items() if word not in common_words\n",
    "        }\n",
    "        exclusive_false = {\n",
    "            word: freq for word, freq in filtered_false.items() if word not in common_words\n",
    "        }\n",
    "\n",
    "        # Opcional: ordenar por frecuencia descendente\n",
    "        filtered_true = dict(sorted(filtered_true.items(), key=lambda x: x[1], reverse=True))\n",
    "        filtered_false = dict(sorted(filtered_false.items(), key=lambda x: x[1], reverse=True))\n",
    "        exclusive_true = dict(sorted(exclusive_true.items(), key=lambda x: x[1], reverse=True))\n",
    "        exclusive_false = dict(sorted(exclusive_false.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        # Guarda resultados\n",
    "        base = f\"nb{min_freq_true}_na{int(max_prop * 100)}\"\n",
    "        with open(f\"json/dictionaries/true_{base}.json\", \"w\") as f:\n",
    "            json.dump(filtered_true, f, indent=2)\n",
    "        with open(f\"json/dictionaries/false_{base}.json\", \"w\") as f:\n",
    "            json.dump(filtered_false, f, indent=2)\n",
    "        with open(f\"json/dictionaries/true_exclusive_{base}.json\", \"w\") as f:\n",
    "            json.dump(exclusive_true, f, indent=2)\n",
    "        with open(f\"json/dictionaries/false_exclusive_{base}.json\", \"w\") as f:\n",
    "            json.dump(exclusive_false, f, indent=2)\n",
    "\n",
    "        print(f\"✔ Guardado con min_freq_true={min_freq_true}, min_freq_false={min_freq_false}, max_prop={max_prop}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec30759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "from math import sqrt\n",
    "from math import exp\n",
    "\n",
    "# 10, 20, 40, 60 ,80,100\n",
    "# Cargar diccionarios\n",
    "# nb = [10, 20, 40, 60 ,80,100]\n",
    "nb = min_freq_values\n",
    "\n",
    "\n",
    "diccionario_true = []\n",
    "diccionario_false = []\n",
    "\n",
    "# Logaritmos\n",
    "# for n in nb:\n",
    "#     diccionario_true.append((f\"json/dictionaries/true_nb{n}_na40.json\", 1/log(n)))\n",
    "#     diccionario_false.append((f\"json/dictionaries/false_nb{n}_na40.json\", 1/log(n)))\n",
    "\n",
    "def calculate_weights(nb):\n",
    "    \"\"\"\n",
    "    Calcula los pesos para los diccionarios true y false basados en los valores de nb.\n",
    "\n",
    "    Args:\n",
    "        nb (list): Lista de valores mínimos de frecuencia.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Dos listas de tuplas con rutas de diccionarios y sus pesos correspondientes.\n",
    "    \"\"\"\n",
    "    diccionario_true = []\n",
    "    diccionario_false = []\n",
    "\n",
    "    for n in nb:\n",
    "        # Logaritmos normalizados\n",
    "        # weight = 1 - log(n) / log(max(nb))\n",
    "        \n",
    "        # Raiz de logaritmos\n",
    "        # weight = 1 / sqrt(log(n))\n",
    "\n",
    "        # Exponencial negativa:\n",
    "        # weight = exp(-n / max(nb))\n",
    "        \n",
    "        # Inversa al cuadrado:\n",
    "        # weight = 1 / (n ** 2)\n",
    "\n",
    "        # Peso por rareza relativa: 67,9\n",
    "        weight = 1 / (n - min(nb) + 1)\n",
    "\n",
    "        # Escalado lineal invertido:\n",
    "        # weight = (max(nb) - n) / (max(nb) - min(nb))\n",
    "\n",
    "        # Escalado logístico:\n",
    "\n",
    "\n",
    "        diccionario_true.append((f\"json/dictionaries/true_nb{n}_na40.json\", weight))\n",
    "        diccionario_false.append((f\"json/dictionaries/false_nb{n}_na40.json\", weight))\n",
    "    \n",
    "    return diccionario_true, diccionario_false\n",
    "\n",
    "\n",
    "# Llamar a la función\n",
    "diccionario_true, diccionario_false = calculate_weights(nb)\n",
    "\n",
    "\n",
    "def construir_diccionario_ponderado(lista_archivos_y_pesos):\n",
    "    resultado = defaultdict(float)\n",
    "    for ruta, peso in lista_archivos_y_pesos:\n",
    "        with open(ruta, \"r\") as f:\n",
    "            datos = json.load(f)\n",
    "            for palabra in datos:\n",
    "                resultado[palabra] += peso\n",
    "    return dict(resultado)\n",
    "\n",
    "# Crear diccionarios ponderados\n",
    "diccionario_true_ponderado = construir_diccionario_ponderado(diccionario_true)\n",
    "diccionario_false_ponderado = construir_diccionario_ponderado(diccionario_false)\n",
    "\n",
    "# Opcional: guardar resultados\n",
    "with open(\"json/dictionaries_ponderados/diccionario_true_ponderado.json\", \"w\") as f:\n",
    "    json.dump(diccionario_true_ponderado, f, indent=2)\n",
    "\n",
    "with open(\"json/dictionaries_ponderados/diccionario_false_ponderado.json\", \"w\") as f:\n",
    "    json.dump(diccionario_false_ponderado, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62bfb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Cargar los diccionarios ponderados\n",
    "with open(\"json/dictionaries_ponderados/diccionario_true_ponderado.json\") as f:\n",
    "    diccionario_true_ponderado = json.load(f)\n",
    "\n",
    "with open(\"json/dictionaries_ponderados/diccionario_false_ponderado.json\") as f:\n",
    "    diccionario_false_ponderado = json.load(f)\n",
    "\n",
    "\n",
    "# Función para calcular el valor de una statement\n",
    "def calcular_score(statement):\n",
    "    # Preprocesar: pasar a minúsculas, quitar puntuación\n",
    "    palabras = re.findall(r\"\\b\\w+\\b\", statement.lower())\n",
    "    score = 0.0\n",
    "    for palabra in palabras:\n",
    "        score += diccionario_true_ponderado.get(palabra, 0)\n",
    "        score -= diccionario_false_ponderado.get(palabra, 0)\n",
    "    return score\n",
    "\n",
    "# Aplicar la función al dataset\n",
    "# df[\"statement_score\"] = df[\"statement\"].apply(calcular_score)\n",
    "# df_train_exportado_words_onehot[\"statement_score\"] = df_train_exportado_words_onehot[statement].apply(calcular_score)\n",
    "# df_test_exportado_words_onehot[\"statement_score\"] = df_test_exportado_words_onehot[statement].apply(calcular_score)\n",
    "df_train_exportado_words_onehot[\"statement_score\"] = df_train_exportado[statement].apply(calcular_score)\n",
    "df_test_exportado_words_onehot[\"statement_score\"] = df_test_exportado[statement].apply(calcular_score)\n",
    "\n",
    "print(df_train_exportado_words_onehot.head(5))\n",
    "# Guardar el dataset con la nueva columna (opcional)\n",
    "# df.to_csv(\"data/train_con_score.csv\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5a4e60c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "df_train_exportado_words_onehot[\"statement_score_bin\"] = df_train_exportado_words_onehot[\"statement_score\"].apply(lambda x: 1 if x > 0 else 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6c3abeb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Comparar predicciones con etiquetas reales\n",
    "comparacion = df_train_exportado_words_onehot[[\"label\", \"statement_score_bin\"]]\n",
    "\n",
    "# Verdaderos positivos (predijo 1 y es 1)\n",
    "tp = ((comparacion[\"label\"] == 1) & (comparacion[\"statement_score_bin\"] == 1)).sum()\n",
    "\n",
    "# Verdaderos negativos (predijo 0 y es 0)\n",
    "tn = ((comparacion[\"label\"] == 0) & (comparacion[\"statement_score_bin\"] == 0)).sum()\n",
    "\n",
    "# Falsos positivos (predijo 1 pero es 0)\n",
    "fp = ((comparacion[\"label\"] == 0) & (comparacion[\"statement_score_bin\"] == 1)).sum()\n",
    "\n",
    "# Falsos negativos (predijo 0 pero es 1)\n",
    "fn = ((comparacion[\"label\"] == 1) & (comparacion[\"statement_score_bin\"] == 0)).sum()\n",
    "\n",
    "print(\"Resultados de la comparación:\")\n",
    "print(f\"Verdaderos positivos (1 predicho, 1 real): {tp}\")\n",
    "print(f\"Falsos positivos     (0 predicho, 1 real): {fn}\")\n",
    "print(f\"Verdaderos negativos (0 predicho, 0 real): {tn}\")\n",
    "print(f\"Falsos negativos     (1 predicho, 0 real): {fp}\")\n",
    "\n",
    "buenos = tp + tn\n",
    "malos = fp + fn\n",
    "\n",
    "print(f\"Porcentaje de buenos: {buenos / (buenos + malos) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0209f5",
   "metadata": {},
   "source": [
    "# 99 Busqueda de mejor manera de evaluar\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f75f2967",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "example_frec_values = [\n",
    "    # [4,7,10 ,20, 40, 60 ,80,100],       # 67,65\n",
    "    # [4,7,10 ,20, 35, 55 ,80,105, 135],     # 67,89\n",
    "    # [4,7,10 ,20, 32, 55 ,80,105, 135],     # 67,89\n",
    "    # [4,7,10 ,16, 26, 38, 56 ,80,105, 135],     # 68,58\n",
    "    # [4,7,10 ,16, 26, 38, 56 ,80,105, 135, 160],   # 68,84\n",
    "    # [4,7,10 ,16, 26, 38, 56 ,80,105, 135, 150, 170],   # 69.12\n",
    "    # [4,7,10 ,16, 26, 38, 56 ,80,105, 135, 150, 170, 200],   # 69.14\n",
    "    # [4,7,10 ,16, 26, 38, 56 ,80,105, 135, 150, 170, 200, 230, 260], # 69.26 \n",
    "    # [4,7,10 ,14, 19, 26, 32, 38, 56 ,80,105, 135, 150, 170, 200, 230, 270 ], # 69,58 \n",
    "    # [4,7,10,14, 19, 26, 32, 38, 56 ,80,105, 135, 150, 170, 200, 230, 270 ],  \n",
    "    mi_array\n",
    "    # [4,7,10 ,20, 35, 55 ,80,105],     # 67,55\n",
    "    # [4,7,10 ,20, 40],\n",
    "    # [4,7,10 ,20, 40, 60 ],              # 67,32\n",
    "    # [4,7,10 ,20, 40, 60 , 80],          # 67,55\n",
    "    # [4,7,10 ,20, 60, 80 , 100, 120],    # 66,99\n",
    "\n",
    "    # [4,6,8,10 ,20, 40, 60 ,80,100],\n",
    "    # [4,6,8,10 ,20, 60,100],\n",
    "    # [5,7,10 ,20, 40, 60 ,80,100],\n",
    "    # [4,7,11 ,16, 22, 29 ,37,45],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8008284",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import json\n",
    "from math import log\n",
    "import pandas as pd\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for example_min_freq_values in example_frec_values:\n",
    "    result_entry = {\n",
    "        \"frequencies\": example_min_freq_values,\n",
    "        \"results\": []\n",
    "    }\n",
    "\n",
    "    for min_freq_true in example_min_freq_values:\n",
    "        min_freq_false = max(1, round(min_freq_true / adjust_ratio))\n",
    "\n",
    "        for max_prop in max_prop_values:\n",
    "            filtered_true = filtra(counter_true, df_true, n_docs_true, min_freq_true, max_prop)\n",
    "            filtered_false = filtra(counter_false, df_false, n_docs_false, min_freq_false, max_prop)\n",
    "\n",
    "            common_words = set(filtered_true) & set(filtered_false)\n",
    "            exclusive_true = {w: f for w, f in filtered_true.items() if w not in common_words}\n",
    "            exclusive_false = {w: f for w, f in filtered_false.items() if w not in common_words}\n",
    "\n",
    "            filtered_true = dict(sorted(filtered_true.items(), key=lambda x: x[1], reverse=True))\n",
    "            filtered_false = dict(sorted(filtered_false.items(), key=lambda x: x[1], reverse=True))\n",
    "            exclusive_true = dict(sorted(exclusive_true.items(), key=lambda x: x[1], reverse=True))\n",
    "            exclusive_false = dict(sorted(exclusive_false.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "            base = f\"nb{min_freq_true}_na{int(max_prop * 100)}\"\n",
    "            with open(f\"json/dictionaries/true_{base}.json\", \"w\") as f:\n",
    "                json.dump(filtered_true, f, indent=2)\n",
    "            with open(f\"json/dictionaries/false_{base}.json\", \"w\") as f:\n",
    "                json.dump(filtered_false, f, indent=2)\n",
    "            with open(f\"json/dictionaries/true_exclusive_{base}.json\", \"w\") as f:\n",
    "                json.dump(exclusive_true, f, indent=2)\n",
    "            with open(f\"json/dictionaries/false_exclusive_{base}.json\", \"w\") as f:\n",
    "                json.dump(exclusive_false, f, indent=2)\n",
    "\n",
    "            print(f\"✔ Guardado con min_freq_true={min_freq_true}, min_freq_false={min_freq_false}, max_prop={max_prop}\")\n",
    "\n",
    "    \n",
    "    nb = example_min_freq_values\n",
    "    diccionario_true, diccionario_false = calculate_weights(example_min_freq_values)\n",
    "\n",
    "    # diccionario_true = [(f\"json/dictionaries/true_nb{n}_na40.json\", 1/log(n)) for n in nb]\n",
    "    # diccionario_false = [(f\"json/dictionaries/false_nb{n}_na40.json\", 1/log(n)) for n in nb]\n",
    "\n",
    "    diccionario_true_ponderado = construir_diccionario_ponderado(diccionario_true)\n",
    "    diccionario_false_ponderado = construir_diccionario_ponderado(diccionario_false)\n",
    "\n",
    "    with open(\"diccionario_true_ponderado.json\", \"w\") as f:\n",
    "        json.dump(diccionario_true_ponderado, f, indent=2)\n",
    "    with open(\"diccionario_false_ponderado.json\", \"w\") as f:\n",
    "        json.dump(diccionario_false_ponderado, f, indent=2)\n",
    "\n",
    "    with open(\"diccionario_true_ponderado.json\") as f:\n",
    "        diccionario_true_ponderado = json.load(f)\n",
    "    with open(\"diccionario_false_ponderado.json\") as f:\n",
    "        diccionario_false_ponderado = json.load(f)\n",
    "\n",
    "    df = df_train_exportado\n",
    "    df[\"statement_score\"] = df[\"statement-lemmatize\"].apply(calcular_score)\n",
    "    df.to_csv(\"data/train_con_score.csv\", index=False)\n",
    "\n",
    "    df[\"statement_score_bin\"] = df[\"statement_score\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    comparacion = df[[\"label\", \"statement_score_bin\"]]\n",
    "    tp = ((comparacion[\"label\"] == 1) & (comparacion[\"statement_score_bin\"] == 1)).sum()\n",
    "    tn = ((comparacion[\"label\"] == 0) & (comparacion[\"statement_score_bin\"] == 0)).sum()\n",
    "    fp = ((comparacion[\"label\"] == 0) & (comparacion[\"statement_score_bin\"] == 1)).sum()\n",
    "    fn = ((comparacion[\"label\"] == 1) & (comparacion[\"statement_score_bin\"] == 0)).sum()\n",
    "\n",
    "    buenos = tp + tn\n",
    "    malos = fp + fn\n",
    "    precision = buenos / (buenos + malos) * 100\n",
    "\n",
    "    print(f\"Resultados de la comparación con min_freq_true={min_freq_true}, min_freq_false={min_freq_false}, max_prop={max_prop}\")\n",
    "    print(f\"TP: {tp} | FN: {fn} | TN: {tn} | FP: {fp}\")\n",
    "    print(f\"Porcentaje de buenos: {precision:.2f}%\\n\")\n",
    "\n",
    "    result_entry[\"results\"].append({\n",
    "        \"min_freq_true\": min_freq_true,\n",
    "        \"min_freq_false\": min_freq_false,\n",
    "        \"max_prop\": max_prop,\n",
    "        \"tp\": tp,\n",
    "        \"tn\": tn,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn,\n",
    "        \"precision\": round(precision, 2)\n",
    "    })\n",
    "\n",
    "    all_results.append(result_entry)\n",
    "\n",
    "# Guardar todos los resultados agrupados por array\n",
    "with open(\"json/resultados_totales.json\", \"w\") as f:\n",
    "    # json.dump(all_results, f, indent=2)\n",
    "    json.dump(all_results, f, indent=2, default=str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1b56eb",
   "metadata": {},
   "source": [
    "# 3.5"
   ]
  },
  {
   "cell_type": "raw",
   "id": "862d6b40",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "df_test_submit = df_test_exportado[['id','statement-lemmatize']].copy()\n",
    "df_test_submit[statement] = clean_statements(df_test_submit['statement-lemmatize']) \n",
    "\n",
    "df_test_submit[\"label\"] = df_test_submit[statement].apply(calcular_score)\n",
    "\n",
    "df_test_submit[\"label\"] = df_test_submit[\"label\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "\n",
    "# drop df_test_submit[statement]\n",
    "df_test_submit = df_test_submit.drop(columns=[statement])\n",
    "\n",
    "df_test_submit.to_csv('./submit/submit_nlp3.csv', index=False)\n",
    "print(\"Archivo CSV de test exportado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8858c31f",
   "metadata": {},
   "source": [
    "# 4. Puntuacíón por diccionarios "
   ]
  },
  {
   "cell_type": "raw",
   "id": "db5780b3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "dictionary_true = 'true_exclusive_nb20_na80'\n",
    "dictionary_false = 'false_exclusive_nb20_na80'\n",
    "\n",
    "# Carga los diccionarios true y false\n",
    "with open(f\"json/dictionaries/{dictionary_true}.json\", 'r') as f:\n",
    "    dict_true = json.load(f)\n",
    "\n",
    "with open(f\"json/dictionaries/{dictionary_false}.json\", 'r') as f:\n",
    "    dict_false = json.load(f)\n",
    "\n",
    "# Combina ambos diccionarios\n",
    "complete_dictionary = set({**dict_true, **dict_false}.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a230f267",
   "metadata": {},
   "source": [
    "# 4. Creacion de One-Hot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31355414",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ...existing code...\n",
    "\n",
    "def one_hot_encode_statements(statements_raw, selected_words):\n",
    "    \"\"\"\n",
    "    Genera una codificación One-Hot para los statements, basada en un conjunto de palabras seleccionadas.\n",
    "\n",
    "    Args:\n",
    "        statements_raw (list of str): Lista de frases originales.\n",
    "        selected_words (set): Conjunto de palabras seleccionadas.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame One-Hot codificado.\n",
    "    \"\"\"\n",
    "    # Agregar prefijo 'word_' a cada palabra seleccionada\n",
    "    prefixed_words = {f\"word_{word}\" for word in selected_words}\n",
    "\n",
    "    # Inicializa lista para cada fila codificada\n",
    "    encoded_rows = []\n",
    "\n",
    "    for statement in statements_raw:\n",
    "        tokens = set(str(statement).split())  # Convierte en set para rapidez\n",
    "        encoded_row = {f\"word_{word}\": int(word in tokens) for word in selected_words}\n",
    "        encoded_rows.append(encoded_row)\n",
    "\n",
    "    # Crea DataFrame\n",
    "    df_one_hot = pd.DataFrame(encoded_rows)\n",
    "    df_one_hot.fillna(0, inplace=True)  # Por si alguna palabra no se encuentra\n",
    "\n",
    "    return df_one_hot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3b4cc49",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "dictionary_true = 'true_exclusive_nb55_na40.json'\n",
    "dictionary_false = 'false_exclusive_nb55_na40.json'\n",
    "\n",
    "# Carga los diccionarios true y false\n",
    "with open(f\"json/dictionaries/{dictionary_true}\", 'r') as f:\n",
    "    dict_true = json.load(f)\n",
    "\n",
    "with open(f\"json/dictionaries/{dictionary_false}\", 'r') as f:\n",
    "    dict_false = json.load(f)\n",
    "\n",
    "# Combina ambos diccionarios\n",
    "complete_dictionary = set({**dict_true, **dict_false}.keys())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8b3edb4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "df_train_exportado_words_onehot = one_hot_encode_statements(df_train_exportado[statement], complete_dictionary)\n",
    "df_test_exportado_words_onehot = one_hot_encode_statements(df_test_exportado[statement], complete_dictionary)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70004918",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "print(df_train_exportado_words_onehot.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e02321",
   "metadata": {},
   "source": [
    "# 5. One Hot - variables categóricas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41a7434",
   "metadata": {},
   "source": [
    "## 5.0 Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2b7909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_train_exportado.columns)\n",
    "\n",
    "# Filtrar las columnas que comienzan con 'subject_'\n",
    "subject_columns = [col for col in df_train_exportado.columns if col.startswith('subject-')]\n",
    "print(subject_columns)\n",
    "# Agregar las columnas filtradas al DataFrame words_onehot\n",
    "df_train_exportado_words_onehot = pd.concat([df_train_exportado_words_onehot, df_train_exportado[subject_columns]], axis=1)\n",
    "df_test_exportado_words_onehot = pd.concat([df_test_exportado_words_onehot, df_test_exportado[subject_columns]], axis=1)\n",
    "\n",
    "print(\"Columnas 'subject_' añadidas correctamente a los DataFrames words_onehot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581580ae",
   "metadata": {},
   "source": [
    "## 5.1 speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fd7a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_exportado_words_onehot['speaker_grouped'] = df_train_exportado['speaker_grouped']\n",
    "df_test_exportado_words_onehot['speaker_grouped'] = df_test_exportado['speaker_grouped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01686fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_exportado_words_onehot = pd.get_dummies(df_train_exportado_words_onehot, columns=['speaker_grouped'], prefix='speaker_grouped', dtype=int)\n",
    "df_test_exportado_words_onehot = pd.get_dummies(df_test_exportado_words_onehot, columns=['speaker_grouped'], prefix='speaker_grouped', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd957683",
   "metadata": {},
   "source": [
    "## 5.2 speaker_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09528a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_exportado_words_onehot['speaker_job_grouped'] = df_train_exportado['speaker_job_grouped']\n",
    "df_test_exportado_words_onehot['speaker_job_grouped'] = df_test_exportado['speaker_job_grouped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbdac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_exportado_words_onehot = pd.get_dummies(df_train_exportado_words_onehot, columns=['speaker_job_grouped'], prefix='speaker_job_grouped', dtype=int)\n",
    "df_test_exportado_words_onehot = pd.get_dummies(df_test_exportado_words_onehot, columns=['speaker_job_grouped'], prefix='speaker_job_grouped', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fbec43",
   "metadata": {},
   "source": [
    "## 5.3 state_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa1a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_exportado_words_onehot['state_info_grouped'] = df_test_exportado['state_info_grouped']\n",
    "df_train_exportado_words_onehot['state_info_grouped'] = df_train_exportado['state_info_grouped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddd2e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfc = pd.get_dummies(dfc, columns=['state_info_grouped'], prefix='state_info_grouped', dtype=int)\n",
    "df_train_exportado_words_onehot = pd.get_dummies(df_train_exportado_words_onehot, columns=['state_info_grouped'], prefix='state_info_grouped', dtype=int)\n",
    "df_test_exportado_words_onehot = pd.get_dummies(df_test_exportado_words_onehot, columns=['state_info_grouped'], prefix='state_info_grouped', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53e13c0",
   "metadata": {},
   "source": [
    "## 5.4 party_affilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f635e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_exportado_words_onehot['party_group'] = df_test_exportado['party_group']\n",
    "df_train_exportado_words_onehot['party_group'] = df_train_exportado['party_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a88355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfc = pd.get_dummies(dfc, columns=['party_group'], prefix='party_group', dtype=int)\n",
    "df_train_exportado_words_onehot = pd.get_dummies(df_train_exportado_words_onehot, columns=['party_group'], prefix='party_group', dtype=int)\n",
    "df_test_exportado_words_onehot = pd.get_dummies(df_test_exportado_words_onehot, columns=['party_group'], prefix='party_group', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cca170",
   "metadata": {},
   "source": [
    "# 6. Exportar CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322e44f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar las columnas de dfc y dfa\n",
    "columns_train = set(df_train_exportado_words_onehot.columns)\n",
    "columns_test = set(df_test_exportado_words_onehot.columns)\n",
    "\n",
    "# Encontrar las diferencias\n",
    "differences = columns_train.symmetric_difference(columns_test)\n",
    "\n",
    "# Verificar si la única diferencia es 'id' y 'label'\n",
    "if differences == {'id', 'label'}:\n",
    "    print(\"La única diferencia entre las columnas es 'id' y 'label'.\")\n",
    "else:\n",
    "    print(\"Existen otras diferencias en las columnas:\", differences)\n",
    "    print(\"Columnas en train pero no en test:\", columns_train - columns_test)\n",
    "    print(\"Columnas en test pero no en train:\", columns_test - columns_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce0b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar los datasets combinados a archivos CSV\n",
    "df_train_exportado_words_onehot.to_csv('./formated/train_exportado_words.csv', index=False)\n",
    "df_test_exportado_words_onehot.to_csv('./formated/test_exportado_words.csv', index=False)\n",
    "\n",
    "print(\"Datasets combinados y exportados correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13caf815",
   "metadata": {},
   "source": [
    "# 7. Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77052133",
   "metadata": {},
   "source": [
    "* [pandas documentation — pandas 2.2.3 documentation. (s. f.).](https://pandas.pydata.org/docs/)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda3_r2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
