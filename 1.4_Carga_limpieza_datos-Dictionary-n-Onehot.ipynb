{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d999ed5b",
   "metadata": {},
   "source": [
    "<header style=\"width:100%;position:relative\">\n",
    "  <div style=\"width:80%;float:right;\">\n",
    "    <h1>False Political Claim Detection</h1>\n",
    "    <h3>Carga y limpieza de los datos - Diccionario y One Hot</h3>\n",
    "    <h5>Grupo 2</h5>\n",
    "  </div>\n",
    "        <img style=\"width:15%;\" src=\"./images/logo.jpg\" alt=\"UPM\" />\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34585ec",
   "metadata": {},
   "source": [
    "# Índice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee7cb30",
   "metadata": {},
   "source": [
    "1. [Importar Librerias](#1.-Importar-Librerias)  \n",
    "2. [Carga del Dataset](#2.-Carga-del-Dataset)  \n",
    "3. [Configuracion de los diccionarios](#3.-Configuracion-de-los-diccionarios)  \n",
    "4. [Estudio Score](#4.-Estudio-Score)  \n",
    "5. [Creacion de One-Hot](#5.-Creacion-de-One-Hot)  \n",
    "6. [One-Hot Variables Categoricas](#6.-One-Hot-Variables-Categoricas)  \n",
    "7. [Comprobar test](#7.-Comprobar-test)  \n",
    "8. [Exportar CSV](#8.-Exportar-CSV)  \n",
    "9. [Referencias](#9-referencias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69127d82",
   "metadata": {},
   "source": [
    "# 1. Importar Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c8663d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sergio/anaconda3/envs/anaconda3_r2/lib/python3.9/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "from math import log\n",
    "from math import sqrt\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2d083",
   "metadata": {},
   "source": [
    "# 2. Carga del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3ab186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "url = \"formated/train_exportado.csv\" \n",
    "df_train_exportado = pd.read_csv(url)\n",
    "\n",
    "\n",
    "# Test\n",
    "url2 = \"formated/test_exportado.csv\"\n",
    "df_test_exportado = pd.read_csv(url2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4934f5",
   "metadata": {},
   "source": [
    "Creación de nuevo dataset para guardar solo las columnas que nos interesan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaed099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_exportado_words_onehot = df_train_exportado[['id', 'label']]\n",
    "df_test_exportado_words_onehot = df_test_exportado[['id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b52f0",
   "metadata": {},
   "source": [
    "# 3. Configuracion de los diccionarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fbd136",
   "metadata": {},
   "source": [
    "*Statement* selecciada a tratar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d21e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = 'statement-lemmatize'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd29140",
   "metadata": {},
   "source": [
    "Función para limpiar la *statement* y dejar solo las palabras importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25a0d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")  \n",
    "\n",
    "def clean_statements(statements):\n",
    "    global nlp\n",
    "    \"\"\"\n",
    "    Elimina pronombres, palabras vacías, palabras irrelevantes y tokens de longitud 1 de las declaraciones.\n",
    "\n",
    "    Args:\n",
    "        statements (list of str): Lista de declaraciones a limpiar.\n",
    "        nlp (spacy.Language): Modelo de spaCy cargado.\n",
    "\n",
    "    Returns:\n",
    "        list of str: Lista de declaraciones limpias.\n",
    "    \"\"\"\n",
    "    # Combina las stop words de spaCy con las personalizadas\n",
    "    custom_stopwords = {\"i\", \"we\", \"it\", \"he\", \"she\", \"they\", \"our\", \"that\", \"have\", \"be\", \"say\"}\n",
    "    all_stopwords = nlp.Defaults.stop_words.union(custom_stopwords)\n",
    "\n",
    "    cleaned_statements = []\n",
    "    for statement in statements:\n",
    "        doc = nlp(statement)\n",
    "        cleaned_tokens = []\n",
    "        for token in doc:\n",
    "            # Excluir palabras que son stop words, están en la lista personalizada o tienen longitud 1\n",
    "            if token.text.lower() in all_stopwords or token.is_stop or len(token.text) == 1 or len(token.text) == 2:\n",
    "                continue\n",
    "            # Incluir solo palabras relevantes\n",
    "            if token.pos_ in {\"VERB\", \"NOUN\", \"PROPN\", \"ADJ\"}:\n",
    "                cleaned_tokens.append(token.text.lower())\n",
    "        cleaned_statements.append(\" \".join(cleaned_tokens))\n",
    "    return cleaned_statements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba95e976",
   "metadata": {},
   "source": [
    "Limpiar la *statement* de train y test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f90d9794",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_exportado[statement] = clean_statements(df_train_exportado[statement]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7042b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_exportado[statement] = clean_statements(df_test_exportado[statement]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b89089f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        china south china sea military fortress world\n",
      "1    resources execute months iraq war fund expansi...\n",
      "2    wisconsin governor propose tax giveaways corpo...\n",
      "3    representation boyfriend friend family law cas...\n",
      "4    protests wisconsin propose collective bargaini...\n",
      "Name: statement-lemmatize, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_train_exportado[statement].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc7bca",
   "metadata": {},
   "source": [
    "Array de números indicando el número mínimo de veces que aparece en el *dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57aec819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_array_serie(inicio, fin):\n",
    "  \"\"\"\n",
    "  Genera un array que sigue la serie 4, 7, 11, 16, 22... hasta alcanzar o superar el valor final.\n",
    "\n",
    "  Args:\n",
    "    inicio: El primer número de la serie.\n",
    "    fin: El valor máximo que el último elemento del array no debe superar.\n",
    "\n",
    "  Returns:\n",
    "    Una lista (array) con los números de la serie.\n",
    "  \"\"\"\n",
    "  array = [inicio]\n",
    "  ultimo_elemento = inicio\n",
    "#   incremento = 3\n",
    "  incremento = 1\n",
    "  while ultimo_elemento < fin:\n",
    "    ultimo_elemento += incremento\n",
    "    array.append(ultimo_elemento)\n",
    "    incremento += 1\n",
    "  return array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621680c1",
   "metadata": {},
   "source": [
    "*document_frequency*:\n",
    "Esta función calcula la frecuencia de documentos en los que aparece cada palabra única dentro de una lista de documentos tokenizados.\n",
    "\n",
    "*filtra*:\n",
    "Filtra palabras de un diccionario según una frecuencia mínima y una proporción máxima de aparición en los documentos, devolviendo solo las palabras que cumplen ambos criterios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4275e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuenta en cuántos documentos aparece cada palabra\n",
    "def document_frequency(tokenized_docs):\n",
    "    df_counter = Counter()\n",
    "    for doc in tokenized_docs:\n",
    "        unique_words = set(doc)\n",
    "        df_counter.update(unique_words)\n",
    "    return df_counter\n",
    "\n",
    "# Función para filtrar palabras por frecuencia mínima y máxima proporción\n",
    "def filtra(dic_frecuencia, doc_frecuencia, total_docs, min_freq, max_prop):\n",
    "    return {\n",
    "        word: dic_frecuencia[word]  # frecuencia total (recuento)\n",
    "        for word in dic_frecuencia\n",
    "        if doc_frecuencia[word] >= min_freq and (doc_frecuencia[word] / total_docs) <= max_prop\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecb04ee",
   "metadata": {},
   "source": [
    "Selección de parametros para aplicar seleccionar la frecuencia mínima de aparición y la probabilidad máxima de aparición para eliminar palabras que aparecen en demasiadas *statements*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e879d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de valores mínimos de frecuencia. Una palabra debe aparecer al menos este número de veces en los documentos para ser incluida en el diccionario.\n",
    "# min_freq_values = [5, 10, 20, 50]\n",
    "# min_freq_values = [4,7,10 ,20, 35, 55 ,80,105, 135]\n",
    "min_freq_values =  generar_array_serie(4, 270)\n",
    "# mi_array = generar_array_serie(2, 270)\n",
    "\n",
    "# Lista de proporciones máximas. Una palabra no debe aparecer en más del porcentaje especificado de documentos para ser incluida en el diccionario.\n",
    "# max_prop_values = [0.5, 0.65, 0.8]\n",
    "max_prop_values = [0.4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0a7b46",
   "metadata": {},
   "source": [
    "Dividir el *datset* entre los que tienen *label* igual a 0 o 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69f5cbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5795\n",
      "3155\n"
     ]
    }
   ],
   "source": [
    "# Tokenización según clase\n",
    "statements_true = [str(text).split() for text, label in zip(df_train_exportado[statement], df_train_exportado['label']) if label == 1]\n",
    "statements_false = [str(text).split() for text, label in zip(df_train_exportado[statement], df_train_exportado['label']) if label == 0]\n",
    "\n",
    "print(len(statements_true))\n",
    "print(len(statements_false))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48570b82",
   "metadata": {},
   "source": [
    "Sacar la proporción entre la cantidad de filas con *label* igual a 0 o a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ec462df",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs_true = len(statements_true)\n",
    "n_docs_false = len(statements_false)\n",
    "adjust_ratio = n_docs_true / n_docs_false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a4b08f",
   "metadata": {},
   "source": [
    "Se cuentan las palabras únicas en los documentos clasificados como verdaderos  y falsos, y se calcula la frecuencia de documentos en los que aparece cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e4b3a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contadores\n",
    "counter_true = Counter(word for doc in statements_true for word in doc)\n",
    "counter_false = Counter(word for doc in statements_false for word in doc)\n",
    "df_true = document_frequency(statements_true)\n",
    "df_false = document_frequency(statements_false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2606e2",
   "metadata": {},
   "source": [
    "Creación de los diferentes diccionarios de frecuencia de las palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c65c44b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Guardado con min_freq_true=4, min_freq_false=2, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=5, min_freq_false=3, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=7, min_freq_false=4, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=10, min_freq_false=5, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=14, min_freq_false=8, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=19, min_freq_false=10, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=25, min_freq_false=14, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=32, min_freq_false=17, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=40, min_freq_false=22, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=49, min_freq_false=27, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=59, min_freq_false=32, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=70, min_freq_false=38, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=82, min_freq_false=45, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=95, min_freq_false=52, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=109, min_freq_false=59, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=124, min_freq_false=68, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=140, min_freq_false=76, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=157, min_freq_false=85, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=175, min_freq_false=95, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=194, min_freq_false=106, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=214, min_freq_false=117, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=235, min_freq_false=128, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=257, min_freq_false=140, max_prop=0.4\n",
      "✔ Guardado con min_freq_true=280, min_freq_false=152, max_prop=0.4\n"
     ]
    }
   ],
   "source": [
    "# Procesa combinaciones\n",
    "for min_freq_true in min_freq_values:\n",
    "    min_freq_false = max(1, round(min_freq_true / adjust_ratio))  # Ajuste proporcional\n",
    "\n",
    "    for max_prop in max_prop_values:\n",
    "        # Filtrado\n",
    "        filtered_true = filtra(counter_true, df_true, n_docs_true, min_freq_true, max_prop)\n",
    "        filtered_false = filtra(counter_false, df_false, n_docs_false, min_freq_false, max_prop)\n",
    "\n",
    "        # Palabras en común\n",
    "        common_words = set(filtered_true.keys()) & set(filtered_false.keys())\n",
    "\n",
    "        # Exclusivos (frecuencia absoluta)\n",
    "        exclusive_true = {\n",
    "            word: freq for word, freq in filtered_true.items() if word not in common_words\n",
    "        }\n",
    "        exclusive_false = {\n",
    "            word: freq for word, freq in filtered_false.items() if word not in common_words\n",
    "        }\n",
    "\n",
    "        # Opcional: ordenar por frecuencia descendente\n",
    "        filtered_true = dict(sorted(filtered_true.items(), key=lambda x: x[1], reverse=True))\n",
    "        filtered_false = dict(sorted(filtered_false.items(), key=lambda x: x[1], reverse=True))\n",
    "        exclusive_true = dict(sorted(exclusive_true.items(), key=lambda x: x[1], reverse=True))\n",
    "        exclusive_false = dict(sorted(exclusive_false.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        # Guarda resultados\n",
    "        base = f\"nb{min_freq_true}_na{int(max_prop * 100)}\"\n",
    "        # with open(f\"json/dictionaries/true_{base}.json\", \"w\") as f:\n",
    "        #     json.dump(filtered_true, f, indent=2)\n",
    "        # with open(f\"json/dictionaries/false_{base}.json\", \"w\") as f:\n",
    "        #     json.dump(filtered_false, f, indent=2)\n",
    "        with open(f\"json/dictionaries/true_exclusive_{base}.json\", \"w\") as f:\n",
    "            json.dump(exclusive_true, f, indent=2)\n",
    "        with open(f\"json/dictionaries/false_exclusive_{base}.json\", \"w\") as f:\n",
    "            json.dump(exclusive_false, f, indent=2)\n",
    "\n",
    "        print(f\"✔ Guardado con min_freq_true={min_freq_true}, min_freq_false={min_freq_false}, max_prop={max_prop}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d79c8c",
   "metadata": {},
   "source": [
    "Funcíón para calcular el peso de los diferentes diccionarios según su frecuencia de aparición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c224f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_true = []\n",
    "diccionario_false = []\n",
    "\n",
    "def calculate_weights(nb):\n",
    "    \"\"\"\n",
    "    Calcula los pesos para los diccionarios true y false basados en los valores de nb.\n",
    "\n",
    "    Args:\n",
    "        nb (list): Lista de valores mínimos de frecuencia.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Dos listas de tuplas con rutas de diccionarios y sus pesos correspondientes.\n",
    "    \"\"\"\n",
    "    diccionario_true = []\n",
    "    diccionario_false = []\n",
    "\n",
    "    for n in nb:\n",
    "\n",
    "    # Selección del metodo de evaluación de pesosa\n",
    "        ## Logaritmos normalizados\n",
    "        # weight = 1 - log(n) / log(max(nb))\n",
    "        \n",
    "        ## Raiz de logaritmos\n",
    "        # weight = 1 / sqrt(log(n))\n",
    "\n",
    "        ## Exponencial negativa:\n",
    "        # weight = exp(-n / max(nb))\n",
    "        \n",
    "        ## Inversa al cuadrado:\n",
    "        # weight = 1 / (n ** 2)\n",
    "\n",
    "        ## Peso por rareza relativa: 67,9\n",
    "        weight = 1 / (n - min(nb) + 1)\n",
    "\n",
    "        ## Escalado lineal invertido:\n",
    "        # weight = (max(nb) - n) / (max(nb) - min(nb))\n",
    "\n",
    "        diccionario_true.append((f\"json/dictionaries/true_nb{n}_na40.json\", weight))\n",
    "        diccionario_false.append((f\"json/dictionaries/false_nb{n}_na40.json\", weight))\n",
    "    \n",
    "    return diccionario_true, diccionario_false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb91a02d",
   "metadata": {},
   "source": [
    "Función para crear el diccionario ponderado en el que se indica el valor de cada palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "215aaa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construir_diccionario_ponderado(lista_archivos_y_pesos):\n",
    "    resultado = defaultdict(float)\n",
    "    for ruta, peso in lista_archivos_y_pesos:\n",
    "        with open(ruta, \"r\") as f:\n",
    "            datos = json.load(f)\n",
    "            for palabra in datos:\n",
    "                resultado[palabra] += peso\n",
    "    return dict(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d804d01",
   "metadata": {},
   "source": [
    "Calcular el peso de de las palabras en los diccioanrios de true o de false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d47f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamar a la función\n",
    "diccionario_true, diccionario_false = calculate_weights(min_freq_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728321df",
   "metadata": {},
   "source": [
    "Creación de los diccionarios ponderadaos finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ec30759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear diccionarios ponderados\n",
    "diccionario_true_ponderado = construir_diccionario_ponderado(diccionario_true)\n",
    "diccionario_false_ponderado = construir_diccionario_ponderado(diccionario_false)\n",
    "\n",
    "# Opcional: guardar resultados\n",
    "with open(\"json/dictionaries_ponderados/diccionario_true_ponderado.json\", \"w\") as f:\n",
    "    json.dump(diccionario_true_ponderado, f, indent=2)\n",
    "\n",
    "with open(\"json/dictionaries_ponderados/diccionario_false_ponderado.json\", \"w\") as f:\n",
    "    json.dump(diccionario_false_ponderado, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e63d8",
   "metadata": {},
   "source": [
    "Función para calcular el *score* de las diferentes *statement* en función de los diccionarios ponderados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2164b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular el valor de una statement\n",
    "def calcular_score(statement):\n",
    "    # Preprocesar: pasar a minúsculas, quitar puntuación\n",
    "    palabras = re.findall(r\"\\b\\w+\\b\", statement.lower())\n",
    "    score = 0.0\n",
    "    for palabra in palabras:\n",
    "        score += diccionario_true_ponderado.get(palabra, 0)\n",
    "        score -= diccionario_false_ponderado.get(palabra, 0)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f1d48b",
   "metadata": {},
   "source": [
    "Crear los *scores* de los diferentes *statement* del *dataset* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f62bfb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id  label  statement_score\n",
      "0  81f884c64a7      1         1.313367\n",
      "1  30c2723a188      0        -2.288087\n",
      "2  6936b216e5d      0        -0.996092\n",
      "3  b5cd9195738      1        -2.321938\n",
      "4  84f8dac7737      0        -3.824482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_488177/2552788845.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train_exportado_words_onehot[\"statement_score\"] = df_train_exportado[statement].apply(calcular_score)\n",
      "/tmp/ipykernel_488177/2552788845.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test_exportado_words_onehot[\"statement_score\"] = df_test_exportado[statement].apply(calcular_score)\n"
     ]
    }
   ],
   "source": [
    "# Cargar los diccionarios ponderados\n",
    "with open(\"json/dictionaries_ponderados/diccionario_true_ponderado.json\") as f:\n",
    "    diccionario_true_ponderado = json.load(f)\n",
    "\n",
    "with open(\"json/dictionaries_ponderados/diccionario_false_ponderado.json\") as f:\n",
    "    diccionario_false_ponderado = json.load(f)\n",
    "\n",
    "# Aplicar la función al dataset\n",
    "df_train_exportado_words_onehot[\"statement_score\"] = df_train_exportado[statement].apply(calcular_score)\n",
    "df_test_exportado_words_onehot[\"statement_score\"] = df_test_exportado[statement].apply(calcular_score)\n",
    "\n",
    "print(df_train_exportado_words_onehot.head(5))\n",
    "# Guardar el dataset con la nueva columna (opcional)\n",
    "# df.to_csv(\"data/train_con_score.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ccf3c1",
   "metadata": {},
   "source": [
    "Calcular el porentaje de aciertos en *train* según los *scores*"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6c3abeb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Comparar predicciones con etiquetas reales\n",
    "comparacion = df_train_exportado_words_onehot[[\"label\", \"statement_score_bin\"]]\n",
    "\n",
    "# Verdaderos positivos (predijo 1 y es 1)\n",
    "tp = ((comparacion[\"label\"] == 1) & (comparacion[\"statement_score_bin\"] == 1)).sum()\n",
    "\n",
    "# Verdaderos negativos (predijo 0 y es 0)\n",
    "tn = ((comparacion[\"label\"] == 0) & (comparacion[\"statement_score_bin\"] == 0)).sum()\n",
    "\n",
    "# Falsos positivos (predijo 1 pero es 0)\n",
    "fp = ((comparacion[\"label\"] == 0) & (comparacion[\"statement_score_bin\"] == 1)).sum()\n",
    "\n",
    "# Falsos negativos (predijo 0 pero es 1)\n",
    "fn = ((comparacion[\"label\"] == 1) & (comparacion[\"statement_score_bin\"] == 0)).sum()\n",
    "\n",
    "print(\"Resultados de la comparación:\")\n",
    "print(f\"Verdaderos positivos (1 predicho, 1 real): {tp}\")\n",
    "print(f\"Falsos positivos     (0 predicho, 1 real): {fn}\")\n",
    "print(f\"Verdaderos negativos (0 predicho, 0 real): {tn}\")\n",
    "print(f\"Falsos negativos     (1 predicho, 0 real): {fp}\")\n",
    "\n",
    "buenos = tp + tn\n",
    "malos = fp + fn\n",
    "\n",
    "print(f\"Porcentaje de buenos: {buenos / (buenos + malos) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0209f5",
   "metadata": {},
   "source": [
    "# 4 Estudio score\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f75f2967",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "example_frec_values = [\n",
    "    # [4,7,10 ,20, 40, 60 ,80,100],       # 67,65\n",
    "    # [4,7,10 ,20, 35, 55 ,80,105, 135],     # 67,89\n",
    "    # [4,7,10 ,20, 32, 55 ,80,105, 135],     # 67,89\n",
    "    # [4,7,10 ,16, 26, 38, 56 ,80,105, 135],     # 68,58\n",
    "    # [4,7,10 ,16, 26, 38, 56 ,80,105, 135, 160],   # 68,84\n",
    "    # [4,7,10 ,16, 26, 38, 56 ,80,105, 135, 150, 170],   # 69.12\n",
    "    # [4,7,10 ,16, 26, 38, 56 ,80,105, 135, 150, 170, 200],   # 69.14\n",
    "    # [4,7,10 ,16, 26, 38, 56 ,80,105, 135, 150, 170, 200, 230, 260], # 69.26 \n",
    "    # [4,7,10 ,14, 19, 26, 32, 38, 56 ,80,105, 135, 150, 170, 200, 230, 270 ], # 69,58 \n",
    "    # [4,7,10,14, 19, 26, 32, 38, 56 ,80,105, 135, 150, 170, 200, 230, 270 ],  \n",
    "    mi_array\n",
    "    # [4,7,10 ,20, 35, 55 ,80,105],     # 67,55\n",
    "    # [4,7,10 ,20, 40],\n",
    "    # [4,7,10 ,20, 40, 60 ],              # 67,32\n",
    "    # [4,7,10 ,20, 40, 60 , 80],          # 67,55\n",
    "    # [4,7,10 ,20, 60, 80 , 100, 120],    # 66,99\n",
    "\n",
    "    # [4,6,8,10 ,20, 40, 60 ,80,100],\n",
    "    # [4,6,8,10 ,20, 60,100],\n",
    "    # [5,7,10 ,20, 40, 60 ,80,100],\n",
    "    # [4,7,11 ,16, 22, 29 ,37,45],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8008284",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "all_results = []\n",
    "\n",
    "for example_min_freq_values in example_frec_values:\n",
    "    result_entry = {\n",
    "        \"frequencies\": example_min_freq_values,\n",
    "        \"results\": []\n",
    "    }\n",
    "\n",
    "    for min_freq_true in example_min_freq_values:\n",
    "        min_freq_false = max(1, round(min_freq_true / adjust_ratio))\n",
    "\n",
    "        for max_prop in max_prop_values:\n",
    "            filtered_true = filtra(counter_true, df_true, n_docs_true, min_freq_true, max_prop)\n",
    "            filtered_false = filtra(counter_false, df_false, n_docs_false, min_freq_false, max_prop)\n",
    "\n",
    "            common_words = set(filtered_true) & set(filtered_false)\n",
    "            exclusive_true = {w: f for w, f in filtered_true.items() if w not in common_words}\n",
    "            exclusive_false = {w: f for w, f in filtered_false.items() if w not in common_words}\n",
    "\n",
    "            filtered_true = dict(sorted(filtered_true.items(), key=lambda x: x[1], reverse=True))\n",
    "            filtered_false = dict(sorted(filtered_false.items(), key=lambda x: x[1], reverse=True))\n",
    "            exclusive_true = dict(sorted(exclusive_true.items(), key=lambda x: x[1], reverse=True))\n",
    "            exclusive_false = dict(sorted(exclusive_false.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "            base = f\"nb{min_freq_true}_na{int(max_prop * 100)}\"\n",
    "            with open(f\"json/dictionaries/true_{base}.json\", \"w\") as f:\n",
    "                json.dump(filtered_true, f, indent=2)\n",
    "            with open(f\"json/dictionaries/false_{base}.json\", \"w\") as f:\n",
    "                json.dump(filtered_false, f, indent=2)\n",
    "            with open(f\"json/dictionaries/true_exclusive_{base}.json\", \"w\") as f:\n",
    "                json.dump(exclusive_true, f, indent=2)\n",
    "            with open(f\"json/dictionaries/false_exclusive_{base}.json\", \"w\") as f:\n",
    "                json.dump(exclusive_false, f, indent=2)\n",
    "\n",
    "            print(f\"✔ Guardado con min_freq_true={min_freq_true}, min_freq_false={min_freq_false}, max_prop={max_prop}\")\n",
    "\n",
    "    \n",
    "    nb = example_min_freq_values\n",
    "    diccionario_true, diccionario_false = calculate_weights(example_min_freq_values)\n",
    "\n",
    "    # diccionario_true = [(f\"json/dictionaries/true_nb{n}_na40.json\", 1/log(n)) for n in nb]\n",
    "    # diccionario_false = [(f\"json/dictionaries/false_nb{n}_na40.json\", 1/log(n)) for n in nb]\n",
    "\n",
    "    diccionario_true_ponderado = construir_diccionario_ponderado(diccionario_true)\n",
    "    diccionario_false_ponderado = construir_diccionario_ponderado(diccionario_false)\n",
    "\n",
    "    with open(\"diccionario_true_ponderado.json\", \"w\") as f:\n",
    "        json.dump(diccionario_true_ponderado, f, indent=2)\n",
    "    with open(\"diccionario_false_ponderado.json\", \"w\") as f:\n",
    "        json.dump(diccionario_false_ponderado, f, indent=2)\n",
    "\n",
    "    with open(\"diccionario_true_ponderado.json\") as f:\n",
    "        diccionario_true_ponderado = json.load(f)\n",
    "    with open(\"diccionario_false_ponderado.json\") as f:\n",
    "        diccionario_false_ponderado = json.load(f)\n",
    "\n",
    "    df = df_train_exportado\n",
    "    df[\"statement_score\"] = df[\"statement-lemmatize\"].apply(calcular_score)\n",
    "    df.to_csv(\"data/train_con_score.csv\", index=False)\n",
    "\n",
    "    df[\"statement_score_bin\"] = df[\"statement_score\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    comparacion = df[[\"label\", \"statement_score_bin\"]]\n",
    "    tp = ((comparacion[\"label\"] == 1) & (comparacion[\"statement_score_bin\"] == 1)).sum()\n",
    "    tn = ((comparacion[\"label\"] == 0) & (comparacion[\"statement_score_bin\"] == 0)).sum()\n",
    "    fp = ((comparacion[\"label\"] == 0) & (comparacion[\"statement_score_bin\"] == 1)).sum()\n",
    "    fn = ((comparacion[\"label\"] == 1) & (comparacion[\"statement_score_bin\"] == 0)).sum()\n",
    "\n",
    "    buenos = tp + tn\n",
    "    malos = fp + fn\n",
    "    precision = buenos / (buenos + malos) * 100\n",
    "\n",
    "    print(f\"Resultados de la comparación con min_freq_true={min_freq_true}, min_freq_false={min_freq_false}, max_prop={max_prop}\")\n",
    "    print(f\"TP: {tp} | FN: {fn} | TN: {tn} | FP: {fp}\")\n",
    "    print(f\"Porcentaje de buenos: {precision:.2f}%\\n\")\n",
    "\n",
    "    result_entry[\"results\"].append({\n",
    "        \"min_freq_true\": min_freq_true,\n",
    "        \"min_freq_false\": min_freq_false,\n",
    "        \"max_prop\": max_prop,\n",
    "        \"tp\": tp,\n",
    "        \"tn\": tn,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn,\n",
    "        \"precision\": round(precision, 2)\n",
    "    })\n",
    "\n",
    "    all_results.append(result_entry)\n",
    "\n",
    "# Guardar todos los resultados agrupados por array\n",
    "with open(\"json/resultados_totales.json\", \"w\") as f:\n",
    "    # json.dump(all_results, f, indent=2)\n",
    "    json.dump(all_results, f, indent=2, default=str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a230f267",
   "metadata": {},
   "source": [
    "# 5. Creacion de One-Hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee48c9",
   "metadata": {},
   "source": [
    "Crear One-Hot de las 60 palabrás más importantes "
   ]
  },
  {
   "cell_type": "raw",
   "id": "31355414",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def one_hot_encode_statements(statements_raw, selected_words):\n",
    "    \"\"\"\n",
    "    Genera una codificación One-Hot para los statements, basada en un conjunto de palabras seleccionadas.\n",
    "\n",
    "    Args:\n",
    "        statements_raw (list of str): Lista de frases originales.\n",
    "        selected_words (set): Conjunto de palabras seleccionadas.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame One-Hot codificado.\n",
    "    \"\"\"\n",
    "    # Agregar prefijo 'word_' a cada palabra seleccionada\n",
    "    prefixed_words = {f\"word_{word}\" for word in selected_words}\n",
    "\n",
    "    # Inicializa lista para cada fila codificada\n",
    "    encoded_rows = []\n",
    "\n",
    "    for statement in statements_raw:\n",
    "        tokens = set(str(statement).split())  # Convierte en set para rapidez\n",
    "        encoded_row = {f\"word_{word}\": int(word in tokens) for word in selected_words}\n",
    "        encoded_rows.append(encoded_row)\n",
    "\n",
    "    # Crea DataFrame\n",
    "    df_one_hot = pd.DataFrame(encoded_rows)\n",
    "    df_one_hot.fillna(0, inplace=True)  # Por si alguna palabra no se encuentra\n",
    "\n",
    "    return df_one_hot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3b4cc49",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "dictionary_true = 'true_exclusive_nb55_na40.json'\n",
    "dictionary_false = 'false_exclusive_nb55_na40.json'\n",
    "\n",
    "# Carga los diccionarios true y false\n",
    "with open(f\"json/dictionaries/{dictionary_true}\", 'r') as f:\n",
    "    dict_true = json.load(f)\n",
    "\n",
    "with open(f\"json/dictionaries/{dictionary_false}\", 'r') as f:\n",
    "    dict_false = json.load(f)\n",
    "\n",
    "# Combina ambos diccionarios\n",
    "complete_dictionary = set({**dict_true, **dict_false}.keys())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8b3edb4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "df_train_exportado_words_onehot = one_hot_encode_statements(df_train_exportado[statement], complete_dictionary)\n",
    "df_test_exportado_words_onehot = one_hot_encode_statements(df_test_exportado[statement], complete_dictionary)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70004918",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "print(df_train_exportado_words_onehot.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e02321",
   "metadata": {},
   "source": [
    "# 6. One-Hot Variables Categoricas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb62e1e",
   "metadata": {},
   "source": [
    "Creación de los One-Hot de las columnas categóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41a7434",
   "metadata": {},
   "source": [
    "## 6.1 Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a2b7909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subject-ciencia_y_tecnologia', 'subject-economia_y_finanzas', 'subject-educacion', 'subject-medio_ambiente_y_energia', 'subject-otros', 'subject-politica_y_gobierno', 'subject-relaciones_internacionales', 'subject-salud_y_bienestar', 'subject-seguridad_y_justicia', 'subject-sociedad_y_cultura', 'subject-trabajo_y_empleo']\n",
      "Columnas 'subject_' añadidas correctamente a los DataFrames words_onehot.\n"
     ]
    }
   ],
   "source": [
    "# print(df_train_exportado.columns)\n",
    "\n",
    "# Filtrar las columnas que comienzan con 'subject_'\n",
    "subject_columns = [col for col in df_train_exportado.columns if col.startswith('subject-')]\n",
    "print(subject_columns)\n",
    "# Agregar las columnas filtradas al DataFrame words_onehot\n",
    "df_train_exportado_words_onehot = pd.concat([df_train_exportado_words_onehot, df_train_exportado[subject_columns]], axis=1)\n",
    "df_test_exportado_words_onehot = pd.concat([df_test_exportado_words_onehot, df_test_exportado[subject_columns]], axis=1)\n",
    "\n",
    "print(\"Columnas 'subject_' añadidas correctamente a los DataFrames words_onehot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581580ae",
   "metadata": {},
   "source": [
    "## 6.2 speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05fd7a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_exportado_words_onehot['speaker_grouped'] = df_train_exportado['speaker_grouped']\n",
    "df_test_exportado_words_onehot['speaker_grouped'] = df_test_exportado['speaker_grouped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01686fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_exportado_words_onehot = pd.get_dummies(df_train_exportado_words_onehot, columns=['speaker_grouped'], prefix='speaker_grouped', dtype=int)\n",
    "df_test_exportado_words_onehot = pd.get_dummies(df_test_exportado_words_onehot, columns=['speaker_grouped'], prefix='speaker_grouped', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd957683",
   "metadata": {},
   "source": [
    "## 6.3 speaker_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09528a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_exportado_words_onehot['speaker_job_grouped'] = df_train_exportado['speaker_job_grouped']\n",
    "df_test_exportado_words_onehot['speaker_job_grouped'] = df_test_exportado['speaker_job_grouped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdbdac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_exportado_words_onehot = pd.get_dummies(df_train_exportado_words_onehot, columns=['speaker_job_grouped'], prefix='speaker_job_grouped', dtype=int)\n",
    "df_test_exportado_words_onehot = pd.get_dummies(df_test_exportado_words_onehot, columns=['speaker_job_grouped'], prefix='speaker_job_grouped', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fbec43",
   "metadata": {},
   "source": [
    "## 6.4 state_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfa1a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_exportado_words_onehot['state_info_grouped'] = df_test_exportado['state_info_grouped']\n",
    "df_train_exportado_words_onehot['state_info_grouped'] = df_train_exportado['state_info_grouped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cddd2e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfc = pd.get_dummies(dfc, columns=['state_info_grouped'], prefix='state_info_grouped', dtype=int)\n",
    "df_train_exportado_words_onehot = pd.get_dummies(df_train_exportado_words_onehot, columns=['state_info_grouped'], prefix='state_info_grouped', dtype=int)\n",
    "df_test_exportado_words_onehot = pd.get_dummies(df_test_exportado_words_onehot, columns=['state_info_grouped'], prefix='state_info_grouped', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53e13c0",
   "metadata": {},
   "source": [
    "## 6.5 party_affilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4f635e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_exportado_words_onehot['party_group'] = df_test_exportado['party_group']\n",
    "df_train_exportado_words_onehot['party_group'] = df_train_exportado['party_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2a88355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfc = pd.get_dummies(dfc, columns=['party_group'], prefix='party_group', dtype=int)\n",
    "df_train_exportado_words_onehot = pd.get_dummies(df_train_exportado_words_onehot, columns=['party_group'], prefix='party_group', dtype=int)\n",
    "df_test_exportado_words_onehot = pd.get_dummies(df_test_exportado_words_onehot, columns=['party_group'], prefix='party_group', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b50f7",
   "metadata": {},
   "source": [
    "# 7. Comprobar test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9389c1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'statement_score', 'subject-ciencia_y_tecnologia',\n",
       "       'subject-economia_y_finanzas', 'subject-educacion',\n",
       "       'subject-medio_ambiente_y_energia', 'subject-otros',\n",
       "       'subject-politica_y_gobierno', 'subject-relaciones_internacionales',\n",
       "       'subject-salud_y_bienestar', 'subject-seguridad_y_justicia',\n",
       "       'subject-sociedad_y_cultura', 'subject-trabajo_y_empleo',\n",
       "       'speaker_grouped_barack-obama', 'speaker_grouped_chain-email',\n",
       "       'speaker_grouped_donald-trump', 'speaker_grouped_hillary-clinton',\n",
       "       'speaker_grouped_john-mccain', 'speaker_grouped_marco-rubio',\n",
       "       'speaker_grouped_mitt-romney', 'speaker_grouped_other_speakers',\n",
       "       'speaker_grouped_rick-perry', 'speaker_grouped_rick-scott',\n",
       "       'speaker_grouped_scott-walker',\n",
       "       'speaker_job_grouped_business_professional',\n",
       "       'speaker_job_grouped_federal_legislative_candidate',\n",
       "       'speaker_job_grouped_federal_legislative_leader',\n",
       "       'speaker_job_grouped_federal_legislator_house',\n",
       "       'speaker_job_grouped_federal_legislator_senate',\n",
       "       'speaker_job_grouped_governor',\n",
       "       'speaker_job_grouped_legal_professional',\n",
       "       'speaker_job_grouped_local_education_official',\n",
       "       'speaker_job_grouped_local_executive',\n",
       "       'speaker_job_grouped_other_speaker_job',\n",
       "       'speaker_job_grouped_party_leader',\n",
       "       'speaker_job_grouped_political_advisor',\n",
       "       'speaker_job_grouped_political_organization',\n",
       "       'speaker_job_grouped_president',\n",
       "       'speaker_job_grouped_presidential_candidate',\n",
       "       'speaker_job_grouped_public_communicator',\n",
       "       'speaker_job_grouped_state_executive',\n",
       "       'speaker_job_grouped_state_legislator_house',\n",
       "       'speaker_job_grouped_state_legislator_senate',\n",
       "       'state_info_grouped_arizona', 'state_info_grouped_california',\n",
       "       'state_info_grouped_florida', 'state_info_grouped_georgia',\n",
       "       'state_info_grouped_illinois', 'state_info_grouped_massachusetts',\n",
       "       'state_info_grouped_new jersey', 'state_info_grouped_new york',\n",
       "       'state_info_grouped_ohio', 'state_info_grouped_oregon',\n",
       "       'state_info_grouped_other_state_info',\n",
       "       'state_info_grouped_rhode island', 'state_info_grouped_texas',\n",
       "       'state_info_grouped_virginia', 'state_info_grouped_washington dc',\n",
       "       'state_info_grouped_wisconsin', 'party_group_democrat',\n",
       "       'party_group_independent_None', 'party_group_media',\n",
       "       'party_group_official', 'party_group_organization', 'party_group_other',\n",
       "       'party_group_republican', 'party_group_third_Party'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_exportado_words_onehot.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d2f375",
   "metadata": {},
   "source": [
    "Contador de columnas que más aparecen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5df2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La salida ha sido exportada a 'one_hot_counts.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Seleccionar las columnas de tipo One-Hot (excluyendo 'id' y 'statement_score')\n",
    "one_hot_columns = [col for col in df_test_exportado_words_onehot.columns if col not in ['id', 'statement_score']]\n",
    "\n",
    "# Contar la cantidad de unos en cada columna\n",
    "one_hot_counts = df_test_exportado_words_onehot[one_hot_columns].sum().sort_values()\n",
    "\n",
    "# Convertir la salida a un diccionario\n",
    "one_hot_counts_dict = one_hot_counts.to_dict()\n",
    "\n",
    "# Exportar a un archivo JSON\n",
    "with open('json/one_hot_counts.json', 'w') as json_file:\n",
    "    json.dump(one_hot_counts_dict, json_file, indent=4)\n",
    "\n",
    "print(\"La salida ha sido exportada a 'one_hot_counts.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cca170",
   "metadata": {},
   "source": [
    "# 8. Exportar CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "322e44f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existen otras diferencias en las columnas: {'label'}\n",
      "Columnas en train pero no en test: {'label'}\n",
      "Columnas en test pero no en train: set()\n"
     ]
    }
   ],
   "source": [
    "# Comparar las columnas de dfc y dfa\n",
    "columns_train = set(df_train_exportado_words_onehot.columns)\n",
    "columns_test = set(df_test_exportado_words_onehot.columns)\n",
    "\n",
    "# Encontrar las diferencias\n",
    "differences = columns_train.symmetric_difference(columns_test)\n",
    "\n",
    "# Verificar si la única diferencia es 'id' y 'label'\n",
    "if differences == {'id', 'label'}:\n",
    "    print(\"La única diferencia entre las columnas es 'id' y 'label'.\")\n",
    "else:\n",
    "    print(\"Existen otras diferencias en las columnas:\", differences)\n",
    "    print(\"Columnas en train pero no en test:\", columns_train - columns_test)\n",
    "    print(\"Columnas en test pero no en train:\", columns_test - columns_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dce0b33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets combinados y exportados correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Exportar los datasets combinados a archivos CSV\n",
    "df_train_exportado_words_onehot.to_csv('./formated/train_exportado_words.csv', index=False)\n",
    "df_test_exportado_words_onehot.to_csv('./formated/test_exportado_words.csv', index=False)\n",
    "\n",
    "print(\"Datasets combinados y exportados correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13caf815",
   "metadata": {},
   "source": [
    "# 9. Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77052133",
   "metadata": {},
   "source": [
    "* [pandas documentation — pandas 2.2.3 documentation. (s. f.).](https://pandas.pydata.org/docs/)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda3_r2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
